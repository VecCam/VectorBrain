{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS TO BE USED ONLY AS A REFERENCE FOR THE FINAL CODE\n",
    "### Treat it as a demo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from utils import specimen_train_test_split\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define constants\n",
    "CURR_DIR = os.getcwd()\n",
    "CSV_DIR = os.path.join(CURR_DIR, 'csv')\n",
    "IMAGE_STORE = os.path.join(CURR_DIR, 'images/after_padding_cropped/')\n",
    "DATA_FOLDER = os.path.join(CURR_DIR, 'data')\n",
    "ORGANIZED_DATA_FOLDER = os.path.join(DATA_FOLDER, 'organized_data')\n",
    "TRAIN_FOLDER = os.path.join(ORGANIZED_DATA_FOLDER, 'train')\n",
    "TEST_FOLDER = os.path.join(ORGANIZED_DATA_FOLDER, 'test')\n",
    "\n",
    "COLUMNS_OF_INTEREST = ['Image_name', 'specimen', 'species', 'source_dataset']\n",
    "\n",
    "# Check for existence of required directories and create them if necessary\n",
    "assert os.path.exists(IMAGE_STORE), \"IMAGE_STORE does not exist. Run download_data.ipynb first\"\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.mkdir(DATA_FOLDER)\n",
    "if not os.path.exists(ORGANIZED_DATA_FOLDER):\n",
    "    os.mkdir(ORGANIZED_DATA_FOLDER)\n",
    "if not os.path.exists(TRAIN_FOLDER):\n",
    "    os.mkdir(TRAIN_FOLDER)\n",
    "if not os.path.exists(TEST_FOLDER):\n",
    "    os.mkdir(TEST_FOLDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test CSV structure:\n",
    "- index - index of the image\n",
    "- specimen - specimen name, this is unique for each individual mosquito. Each specimen can have multiple images\n",
    "- Image_name - name of the image (generally of the form: `specimen + number`)\n",
    "- species - one hot encoded species category (Anopheles funestus, Culex etc)\n",
    "- source_dataset - helps direct which folder to select images from in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from M7 and M4 \n",
    "df_M7 = pd.read_csv(os.path.join(CSV_DIR, 'M7_data.csv'))\n",
    "df_M4 = pd.read_csv(os.path.join(CSV_DIR, 'M4_data.csv'))\n",
    "df_steph = pd.read_csv(os.path.join(CSV_DIR, 'steph_march.csv')) # colony bred stephensi specimen imaged in March\n",
    "df_gamb = pd.read_csv(os.path.join(CSV_DIR, 'gambiae_march.csv')) # colony bred gambiae specimen imaged in March\n",
    "\n",
    "df_map = {}\n",
    "df_map['M7'] = df_M7\n",
    "df_map['stephensi_march'] = df_steph\n",
    "df_map['gambiae_march'] = df_gamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual data clean-up step -> Assigning correct species labels\n",
    "\n",
    "Here, I am following a 6 class structure for the species labels. The 6 classes are:\n",
    "- Anopheles funestus\n",
    "- Anopheles gambiae\n",
    "- Anopheles stephensi\n",
    "- Anopheles other\n",
    "- Culex\n",
    "- Mansonia\n",
    "\n",
    "Classes are 0 indexed, so the class labels are: `0, 1, 2, 3, 4, 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_species_specimen_names(df):\n",
    "    df['morphSpecies'] = df['morphSpecies'].str.lower()\n",
    "    # remove rows that are nm\n",
    "    df = df[df['morphSpecies'] != 'nm'].copy()\n",
    "    # remove extra spaces\n",
    "    df['morphSpecies'] = df['morphSpecies'].str.strip()\n",
    "\n",
    "    unique_identifier = 'specimenId_unique' if 'specimenId_unique' in df.columns else 'specimenIdDup'\n",
    "    df['Image_name'] = df[unique_identifier] + \".jpg\"\n",
    "    df['specimen'] = \"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        #get the string before the last underscore\n",
    "        if '_' in row['specimenId']:\n",
    "            df.at[idx,'specimen'] = \"\".join(row[unique_identifier].split('_')[:-1])\n",
    "        else:\n",
    "            df.at[idx,'specimen'] = row[unique_identifier]\n",
    "    return df\n",
    "\n",
    "for k in df_map.keys():\n",
    "    df_map[k][\"species\"] = -1 # this is to make sure that all species columns are filled \n",
    "    df_map[k] = clean_species_specimen_names(df_map[k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is hard-coded. Be carful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that gambiae and stephensi are the only species in their respective datasets\n",
    "df_map['stephensi_march']['morphSpecies'] = 'anopheles stephensi'\n",
    "df_map['gambiae_march']['morphSpecies'] = 'anopheles gambiae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify unique morphSpecies name and decide on the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique morph species: {'anopheles other', 'anopheles gambiae', 'mansonia', 'ziemanni', 'culex', 'anopheles funestus', 'other', 'anopheles stephensi'}\n"
     ]
    }
   ],
   "source": [
    "unique_morph_species = []\n",
    "for k in df_map.keys():\n",
    "    unique_morph_species.extend(df_map[k]['morphSpecies'].unique())\n",
    "\n",
    "print(f\"Unique morph species: {set(unique_morph_species)}\")\n",
    "\n",
    "MORPHSPECIES_TO_ONEHOT_SPECIES = { ## Modify this mapping as needed\n",
    "    'anopheles funestus': 0,\n",
    "    'anopheles gambiae': 1,\n",
    "    'anopheles stephensi': 2,\n",
    "    'anopheles other': 3,\n",
    "    'culex': 4,\n",
    "    'mansonia': 5,\n",
    "    'other': 5,\n",
    "    'ziemanni': 3\n",
    "}\n",
    "\n",
    "# assert that all species are in the mapping\n",
    "for k in unique_morph_species:\n",
    "    assert k in MORPHSPECIES_TO_ONEHOT_SPECIES.keys(), f\"{k} not in mapping, add it to MORPHSPECIES_TO_ONEHOT_SPECIES\"\n",
    "\n",
    "# add onehot species column\n",
    "for k in df_map.keys():\n",
    "    df_map[k]['species'] = df_map[k]['morphSpecies'].map(MORPHSPECIES_TO_ONEHOT_SPECIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating folder structure for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M7 has 3706 rows\n",
      "stephensi_march has 188 rows\n",
      "gambiae_march has 109 rows\n"
     ]
    }
   ],
   "source": [
    "for k in df_map.keys():\n",
    "    df_map[k]['source_dataset'] = k\n",
    "    # create folder k in DATA_FOLDER if it doesn't exist\n",
    "    if not os.path.exists(os.path.join(DATA_FOLDER, k)):\n",
    "        os.mkdir(os.path.join(DATA_FOLDER, k))\n",
    "    print(f\"{k} has {len(df_map[k])} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check that all required columns are present\n",
    "def select_columns(df_map):\n",
    "    cleaned_df_map = {}\n",
    "    for k in df_map.keys():\n",
    "        for col in COLUMNS_OF_INTEREST:\n",
    "            assert col in df_map[k].columns, f\"{col} not in {k}'s columns\"\n",
    "        cleaned_df_map[k] = df_map[k][COLUMNS_OF_INTEREST].copy()\n",
    "    return cleaned_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_map = select_columns(df_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (This next block of code is optional )Data before M4 is from ODK. Here is some short code to replicate the same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0    420\n",
      "1.0    297\n",
      "Name: species, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make a copy of the df_M4 DataFrame\n",
    "df_M4 = df_M4.copy()\n",
    "\n",
    "# rename 'value' and 'mosquito_id' columns\n",
    "df_M4.rename(columns={'value': 'Image_name', 'mosquito_id': 'specimen'}, inplace=True)\n",
    "\n",
    "# map 'morph_id_anopheles_species' column values to 'gambiae' and 'stephensi'\n",
    "species_map = {'gambiae': 1, 'stephensi': 2}\n",
    "df_M4['species'] = df_M4['morph_id_anopheles_species'].map(species_map)\n",
    "\n",
    "# select only the rows with non-null species values\n",
    "df_M4 = df_M4[df_M4['species'].notna()].copy()\n",
    "\n",
    "# add 'source_dataset' column with value 'M4'\n",
    "df_M4['source_dataset'] = 'M4'\n",
    "\n",
    "# print the count of unique species values\n",
    "print(df_M4['species'].value_counts())\n",
    "\n",
    "for col in COLUMNS_OF_INTEREST:\n",
    "    assert col in df_M4.columns, f\"{col} not in df_M4's columns\"\n",
    "\n",
    "# add the M4 DataFrame to the cleaned_df_map\n",
    "cleaned_df_map['M4'] = df_M4[COLUMNS_OF_INTEREST].copy()\n",
    "\n",
    "# create folder M4 in DATA_FOLDER if it doesn't exist\n",
    "if not os.path.exists(os.path.join(DATA_FOLDER, 'M4')):\n",
    "    os.mkdir(os.path.join(DATA_FOLDER, 'M4'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize into source dataset folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing M7 dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3706/3706 [00:03<00:00, 959.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M7 has 3706 rows \n",
      "\n",
      "Processing stephensi_march dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 4924.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stephensi_march has 188 rows \n",
      "\n",
      "Processing gambiae_march dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:00<00:00, 4493.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gambiae_march has 109 rows \n",
      "\n",
      "Processing M4 dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 717/717 [00:00<00:00, 4282.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M4 has 295 rows \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def identify_and_move_valid_images(df):\n",
    "    df[\"Image_Available\"] = True\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_name = row['Image_name']\n",
    "        source_dataset = row['source_dataset']\n",
    "        species = row['species']\n",
    "        specimen = row['specimen']\n",
    "        image_path = os.path.join(IMAGE_STORE, image_name)\n",
    "        # check if the image exists\n",
    "        if not os.path.exists(image_path):\n",
    "            df.at[idx, 'Image_Available'] = False\n",
    "            continue\n",
    "        destination_path = os.path.join(DATA_FOLDER, source_dataset)\n",
    "        # create the destination path if it doesn't exist\n",
    "        if not os.path.exists(destination_path):\n",
    "            os.makedirs(destination_path)\n",
    "        # copy the image to the destination path\n",
    "        shutil.copy(image_path, destination_path)\n",
    "    # select only the rows with valid images\n",
    "    df = df[df['Image_Available']].copy()\n",
    "    return df\n",
    "\n",
    "for k in cleaned_df_map.keys():\n",
    "    print(f\"Processing {k} dataframe\")\n",
    "    cleaned_df_map[k] = identify_and_move_valid_images(cleaned_df_map[k])\n",
    "    print(f\"{k} has {len(cleaned_df_map[k])} rows \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M7 dataframe\n",
      "train_list length: 1427\n",
      "test_list length: 357\n",
      "\n",
      "Processing stephensi_march dataframe\n",
      "train_list length: 40\n",
      "test_list length: 10\n",
      "\n",
      "Processing gambiae_march dataframe\n",
      "train_list length: 31\n",
      "test_list length: 8\n",
      "\n",
      "Processing M4 dataframe\n",
      "train_list length: 88\n",
      "test_list length: 22\n"
     ]
    }
   ],
   "source": [
    "train_test_dict = {}\n",
    "for k in cleaned_df_map.keys():\n",
    "    print(f\"\\nProcessing {k} dataframe\")\n",
    "    output = specimen_train_test_split(cleaned_df_map[k]) # output is a tuple of (train_df, test_df)\n",
    "    train_test_dict[k] = output\n",
    "\n",
    "# join and concatenate the train and test dataframes\n",
    "train_df = pd.concat([train_test_dict[k][0] for k in train_test_dict.keys()])\n",
    "test_df = pd.concat([train_test_dict[k][1] for k in train_test_dict.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the images into the train and test folders (if dataloader uses this structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying images to train folder: 100%|██████████| 3470/3470 [00:02<00:00, 1619.26it/s]\n",
      "Copying images to test folder: 100%|██████████| 828/828 [00:00<00:00, 1604.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# delete everything in the train and test folders\n",
    "for folder in [TRAIN_FOLDER, TEST_FOLDER]:\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.mkdir(folder)\n",
    "    \n",
    "# copy the images to the train and test folders according to the train and test dataframes and create folders for each species as C_{species}\n",
    "def copy_images_to_train_test_folders(df, dest_folder):\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Copying images to {dest_folder.split('/')[-1]} folder\"):\n",
    "        image_name = row['Image_name']\n",
    "        species = row['species']\n",
    "        image_path = os.path.join(DATA_FOLDER, row['source_dataset'], image_name)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"{image_path} doesn't exist\")\n",
    "            continue\n",
    "        destination_path = os.path.join(dest_folder, f\"C_{species}\")\n",
    "        # create the destination path if it doesn't exist\n",
    "        if not os.path.exists(destination_path):\n",
    "            os.makedirs(destination_path)\n",
    "        # copy the image to the destination path\n",
    "        shutil.copy(image_path, destination_path)\n",
    "\n",
    "copy_images_to_train_test_folders(train_df, TRAIN_FOLDER)\n",
    "copy_images_to_train_test_folders(test_df, TEST_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cd3544f732bc4520593c177004321a70fa103a617e9ef66b64ca763584b4c73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
